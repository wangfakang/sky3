{"name":"Sky3","tagline":"nginx的ngx_http_upstream_check_module的解析","body":"`` 对NGX_HTTP_UPSTREAM_DYNAMIC_MODULE的分析：\r\n``\r\n\r\n# 内容： \r\n\r\n## 一：配置说明\r\n\r\n## 二：代码解析\r\n\r\n## 三：总结注意点\r\n\r\n\r\n\r\n\r\n### 一：配置说明\r\n```nginx\r\nhttp {\r\n    lua_package_path \"/path/to/lua-resty-upstream-healthcheck/lib/?.lua;;\";\r\n\r\n    # sample upstream block:\r\n    upstream foo.com {\r\n        server 127.0.0.1:12354;\r\n        server 127.0.0.1:12355;\r\n        server 127.0.0.1:12356 backup;\r\n    }   \r\n\r\n    # the size depends on the number of servers in upstream {}: \r\n    lua_shared_dict healthcheck 1m; \r\n\r\n    lua_socket_log_errors off;\r\n\r\n    init_worker_by_lua '\r\n        local hc = require \"resty.upstream.healthcheck\"\r\n\r\n        local ok, err = hc.spawn_checker{\r\n            shm = \"healthcheck\",  -- defined by \"lua_shared_dict\"\r\n            upstream = \"foo.com\", -- defined by \"upstream\"\r\n            type = \"http\",\r\n\r\n            -- if you put this Lua snippet in separate .lua file,\r\n            -- then you should write this instead: http_req = \"GET /status HTTP/1.0\\r\\nHost: foo.com\\r\\n\\r\\n\",\r\n            http_req = \"GET /status HTTP/1.0\\\\r\\\\nHost: foo.com\\\\r\\\\n\\\\r\\\\n\",\r\n                    -- raw HTTP request for checking\r\n\r\n            interval = 2000,  -- run the check cycle every 2 sec\r\n            timeout = 1000,   -- 1 sec is the timeout for network operations\r\n            fall = 3,  -- # of successive failures before turning a peer down\r\n            rise = 2,  -- # of successive successes before turning a peer up\r\n            valid_statuses = {200, 302},  -- a list valid HTTP status code\r\n            concurrency = 10,  -- concurrency level for test requests\r\n        }\r\n        if not ok then\r\n            ngx.log(ngx.ERR, \"failed to spawn health checker: \", err)\r\n            return\r\n        end\r\n\r\n        -- Just call hc.spawn_checker() for more times here if you have\r\n        -- more upstream groups to monitor. One call for one upstream group.\r\n        -- They can all share the same shm zone without conflicts but they\r\n        -- need a bigger shm zone for obvious reasons.\r\n    ';\r\n\r\n    server {\r\n        ...\r\n\r\n        # status page for all the peers:\r\n        location = /status {\r\n            access_log off;\r\n            allow 127.0.0.1;\r\n            deny all;\r\n\r\n            default_type text/plain;\r\n            content_by_lua '\r\n                local hc = require \"resty.upstream.healthcheck\"\r\n                ngx.say(\"Nginx Worker PID: \", ngx.worker.pid())\r\n                ngx.print(hc.status_page())\r\n            ';\r\n        }\r\n    }\r\n}\r\n```\r\n        \r\n\r\n上面的配置就启用了upstream的健康检查。\r\n\r\n```   \r\n注意此模块是agentzh使用nxg_http_lua_upstream_module中的set_peer_down来实现的健康检查.和tengine实现的是不一样的.\r\n```  \r\n\r\n\r\n细节注意点： \r\n====\r\n* 1.使用nxg_http_lua_upstream_module模块的set_peer_down进行修改的时候，只是对当前work进程生效，但是对于其他的work进程是没有作用的。\r\nagentzh解决方法：\r\n* 2.使用share_dict，在init_work_by_lua阶段为每一个work进程进行设置回调函数定时的去访问share_dict相应的数据进行相应的更新。\r\n\r\n基本原理：  \r\n====\r\n* 1.在init_work_by_lua阶段给每一个work进程设置回调函数，定期的请求相应的后端peer，如果连接失败或是成功则去跟新相应share_dict相应数据(当同一个server连接失败连接失败多次就会把其标记为down，当连续连接成功多少次后就把其标记为非down）\r\n\r\n使用技巧：      \r\n====\r\n* 1.利用share_dict的add函数对于同一个key只可以增加一次的机制，来实现了一个简单的锁,如下：         \r\n\r\n```     \r\n\r\nlocal function get_lock(ctx)\r\n    local dict = ctx.dict\r\n    local key = \"l:\" .. ctx.upstream\r\n\r\n    -- the lock is held for the whole interval to prevent multiple\r\n    -- worker processes from sending the test request simultaneously.\r\n    -- here we substract the lock expiration time by 1ms to prevent\r\n    -- a race condition with the next timer event.     \r\n　  -- 利用其过期机制　相当于设置了一个自动过期的锁         \r\n    local ok, err = dict:add(key, true, ctx.interval - 0.001)\r\n    if not ok then\r\n        if err == \"exists\" then\r\n            return nil \r\n        end\r\n        errlog(\"failed to add key \\\"\", key, \"\\\": \", err)\r\n        return nil \r\n    end \r\n    return true\r\nend\r\n```   \r\n\r\n\r\n下面有几个关键问题：\r\n====\r\n\r\n１.如何保证每一个work子进程只执行一次相应的修改?  　　　　　     \r\n\r\n* 使用了一个版本控制变量version默认为０，为一个upstream设置一个版本控制变量的key，然后value值为其版本数，每次定时回调的时候判断其当前版本是否低于上次版本，是则进行相应的同步。\r\n\r\n2.如何知道哪一个peer进行了相应的修改?  \r\n\r\n*  一旦发现了有新的版本则采用轮训判断每一个id对应的peer。进行相应的key值进行获取。\r\n\r\n\r\n\r\n## 有问题反馈\r\n在使用中有任何问题，欢迎反馈给我，可以用以下联系方式跟我交流\r\n\r\n* 邮件(1031379296#qq.com, 把#换成@)\r\n* QQ: 1031379296\r\n* weibo: [@王发康](http://weibo.com/u/2786211992/home)\r\n\r\n\r\n## 感激\r\n\r\n### chunshengsterATgmail.com\r\n\r\n\r\n## 关于作者\r\n\r\n### Linux\\nginx\\golang\\c\\c++爱好者\r\n### 欢迎一起交流  一起学习# \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}